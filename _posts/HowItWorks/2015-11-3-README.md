---
layout: post
title: How it Works
date: 2015-11-3
categories:
 - how
---

## How it Works (for landing page)

Romana multi-tenant cloud networking is based on a new layer 3 tenancy model that encodes tenant and segment identifiers directly in to the IP address. By combining intelligent IP address management with route control and layer 3 firewalls on the hypervisor, network isolation is enforced between tenants and tenant segments. 

Romana's intelligent IP Address Management (IPAM) system assigns endpoint IP addresses based on the segment that it is part of, as well as the physical host where it is running. This, when deployed in a modern layer 3 routed access datacenter design, allows routes to be aggregated and statically configured on physical network devices. With static routes on the physical network, only the hypervisor needs an updated route when a new endpoint is created. 

Also, since routes are static, no route distribution protocol is necessary, further simplifying network design.

Service insertion is implemented by reconfiguring the default gateway on tenant endpoints to the IP address of the service endpoint.

Further details on how it works are available [here](/how/README/).

Or jump right to the topic you want to learn more abotu

- [Routed Access Datacenter](/how/README/#routed-access-datacenter)   

- [Virtualization hosts](/how/README/#virtualization-hosts) 
 
- [VXLAN Tenant Isolation](/how/README/#vxlan-tenant-isolation)

- [Romana Tenant Isolation](/how/README/#romana-tenant-isolation)
  
- [IP Address Assignment](/how/README/#ip-address-assignment)
  
- [Route Manager](/how/README/#route-manager) 
 
- [Service Insertion](/how/README/#service-insertion) 
 
- [Policy Based Control](/how/README/#policy-based-control)  

---

### Routed Access Datacenter

To completely understand the operation of Romana, it is important to first understand the basics of a layer 3 routed access datacenter design. 

Today's modern datacenters are frequently built using a Spine/Leaf network design. The details of this approach are beyond the scope of this document, but several good resources are available [here]( http://www.cisco.com/c/en/us/products/collateral/switches/nexus-5000-series-switches/guide_c07-673997.html), [here]( https://www.arista.com/assets/data/pdf/DesignGuides/Arista-Universal-Cloud-Network-Design.pdf), and [here](http://searchdatacenter.techtarget.com/feature/Data-center-network-design-moves-from-tree-to-leaf).

These kind of layer 3 designs are popular because of their simplicity and performance and are often used as the underlay network for cloud deployments. The important point to remember about these designs is that they can take advantage of [route aggregation]( http://www.linktionary.com/r/route_aggregation.html) to further simplify their design.

Briefly stated, route aggregation is simply organizing IP addresses and configuring routers in a way that is 'topologically significant'. That is, each part of the network can only contain addresses from a specific CIDR block. By organizing the addresses hierarchically, the number of routes needed to reach every endpoint is reduced. 

The diagram below shows how route aggregation makes all endpoints in the network reachable with just a few routes configured on the leaf and spine devices. It illustrates how a 10/8 network (with up to 16M IP addresses) could be partitioned across 2 Spines and 4 Leaves so that each Spine port forwards packets to one of four /10 networks (one on each Leaf) and each Leaf forward packets to one of 64 ports configured with a /16 network.

![Route Aggregation](/images/Aggregation.png)

With this configuration, each Spine device is configured with the same 4 static routes, one to each Leaf device. Each Leaf device is configured with one route to each port. The routes on the Leafs are sequential and differ across Leaf devices only by an offset in the second octet of the CIDR.

The simplicity of this design is undeniable. It is compact, intuitive and predictable and delivers high performance as well. Which is why it is so popular among large data center operators and the basis for most cloud network underlay designs. 

One constraint imposed by a routed access design is that it *requires endpoint IP addresses to conform to the route aggregation hierarchy*. 

The obvious way to relax this constraint is to update the network devices with individual host routes to the specific endpoints that do not conform to the hierarchy. 

However, updating the devices with host routes introduces the added complexity of a route distribution protocol like BGP, which requires additional skills and training. Additionally, at the limit, injecting host routes, by whatever means, will eventually overflow the capacity of the devices.

### Virtualization Hosts

Allocating a /16 CIDR and 64K IP addresses to a single port does not make a lot of sense until you realize that the host attached to this port would actually be a virtualization host that could use these available IP addresses for VMs or containers running on the host.

For example, on the [Google Compute Engine]( https://cloud.google.com/compute/) IaaS platform, when you launch a Kubernetes VM, by default it is allocated a [complete /24 network]( http://kubernetes.io/v1.0/docs/admin/networking.html#how-to-achieve-this ) so that each pod running would have one of the possible 255 IP addresses from within the assigned range. 

Using this as a simple baseline example, if you allocate a /24 to each VM, having a /16 for the entire virtualization host would accommodate up to 255 VMs.

The diagram below extends the [Routed Access Datacenter](#routed-access-datacenter) design to the virtualization hosts where the host is configured as a router to forward traffic to VMs, where each VM have been allocated a /24 networks for their own use.

![Virtualization Hosts](/images/vHosts.png)

Configuring the host to route traffic is a standard feature built in to the Linux kernel. It simply has to be configured with the routes, just like a network device.

If an actual cloud datacenter were build using the assumptions from the previous example, the 16M IP addresses available in the 10/8 network would be allocated across 255 virtualization hosts, where each could support up to 255 VMs, with each VM supporting up to 255 IP addresses for local endpoints.

Alternative configurations are easily designed simply by changing the CIDR that is assigned to each host. A /18 to each host, would allow for four times as many hosts (up to 1K, 2^8 vs 2^10), but each host could accommodate one fourth the number of endpoints (2^16 v. 2^18).  Other variations include using only a portion of the complete 10/8 for smaller configurations, such as a 10/10 for 4M endpoints, a 10/11 for 2M endpoints, etc.

---

### VXLAN Tenant Isolation

	> This section should probably be on the 'Cloud Native' page, but 
	> for now just including it in line.

The traditional method to maintain tenant isolation is by creating a layer 2 overlay network. These overlays use VXLAN encapsulation (or similar encapsulation protocol) which supports up to 16M isolated network segments. And just like any other layer 2 segment, they are configured with a CIDR address range and gateways to routers that forward traffic to other VLANs and other networks, as necessary. 

Assigning a VXLAN to individual tenant network segments provide VLAN style isolation for every tenant segment. If a NAT service is also available, segments can also support overlapping IP addresses. 

While VXLAN technology is reasonably mature and stable, the performance overhead frequently is identified as one of the major concerns.

VXLAN encapsulation requires every packet be processed to add a header and to insert the correct field values. While line rate throughput can be achieved, the actual performance impact is [difficult to qualify]( http://blog.ipspace.net/2015/02/performance-of-hypervisor-based-overlay.html). However, with header overhead [reducing bandwidth by ~6%](http://packetpushers.net/vxlan-udp-ip-ethernet-bandwidth-overheads/), and [CPU overhead taking at least another 3%](http://chinog.org/wp-content/uploads/2015/05/Optimizing-Your-Virtual-Switch-for-VXLAN.pdf) an optimistic estimate would be that VXLAN introduces at least a 10% performance overhead.   

But more important than the simple stand alone VXLAN peformance benchmarks is the cumulative latency introduced by encap/decap cycles for all east/west traffic and *additional* encap/decap cycles for each intermediate router and/or service functions. 

For example, the diagram below shows the path packets take when OpenStack VMs on different tenant VXLAN segments communicate. There is an extra encap/dcap cycle for the router on a different host to forward the traffic to the proper endpoint.

If 10% performance loss were all it cost to run VXLAN, considering the benefits of segment isolation, it would still seem to be a bargain. However, from an operational perspective, there are other more critical challenges with running VXLANs than the performance impact.

1. *Visibility*: VXLAN Encapsulation hides traffic inside of IP packets that can not be seen with standard network monitoring tools. Actual paths through the physical network are difficult to because endpoints inside VXLANs could be located anywhere. 

2. *Manageability*: VXLANs use a VLAN Network Identifier (VNID) to identify which VLAN the traffic belongs to. Managing these VNID is conceptually similar to managing VLAN ID, except instead of managing 4K ID, you need a system that can handle up to 16M. 

3. *Interoperability*: When running VXLAN, operators will invariably require VTEPs on visualization hosts, network devices and access gateways using different APIs, versions and/or originating from different vendors.  There are no interoperability standards for VTEP, which makes coordinating VNIDs, even with a commercial Enterprise SDN solutions, especially challenging.

4. *Cost*: To avoid the latency impact of encapsulation, network devices that have hardware acceleration for VTEPs may be required. In addition, an SDN solution of some kind is necessary to coordinate VNIDs. There are also the cost of added complexity in operating a VXLAN overlay for the reasons s

![OpenStack VXLAN Routing](/images/vxlan.png)

The operational complexity of running VXLANs have operators looking for ways to provide secure tenant isolation, without the operational and performance penalty of VXLAN.

### Romana Tenant Isolation

Ideally, from an operational perspective, what is missing is a multi-tenant network isolation model that of the routed access datacenter design

  With multiple tenant endpoints on each hypervisor there needs to be a way to ensure that only endpoints that should communicate, are able to communicate. This is achieved by configuring IP tables rules on the hypervisor that maintains a whitelist of the IP addresses that each endpoint can communicate with. [how is this done? By which component?]

Maintaining a list of all endpoints can communicate with all other endpoints is large state management problem. The problem becomes even more difficult when more advanced policies for traffic management need to be implemented.

To further simplify the design, reduce the amount of data to be maintained and to facilitate policy based traffic control, Romana encodes a tenant identifier into IP addresses. Futhermore, the tenant ID is located in the high order bits of the address so that all traffic associated with a tenant can be controlled by establishing L3 traffic management policies identified by the tenant CIDR.

End pop out

---

### IP Address Assignment and Management

To preserve the simplicity of the layer 3 network design and avoid the complexity of running a route distribution protocol for route updates, Romana provides an IP Address Management system that coordinates with cloud orchestration systems such as OpenStack and Kubernetes to provide IP addresses to new endpoints that conform to the data center's address hierarchy. 


### Route Manager

The Route Manager

### Service Insertion

The Service Insertion

### Policy Based Control
The policy controller


