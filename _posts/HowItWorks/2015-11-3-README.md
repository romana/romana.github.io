---
layout: post
title: How it Works
date: 2015-11-3
categories:
 - how
---

[How it Works: tl;dr](#tl;dr)  
[How it Works](#howitworks) 

## How it Works, tl;dr

Romana enables multi-tenant cloud networking by combining intelligent IP address management with route control and layer 3 firewalls to enforce network isolation between tenants and tenant segments.

Romana's intelligent IP Address Management (IPAM) system assigns tenant IP addresses so that routes to these endpoints can be aggregated and statically configured on physical network devices. With static routes on the physical network, only the hypervisor needs an updated route when a new endpoint is created. 

Also, since routes are static no route distribution protocol is necessary, further simplifying network design.

Segment isolation is maintained with IP table rules set on the hypervisor. Service insertion is implemented by reconfiguring the default gateway on tenant endpoints to the IP address of the service endpoint.

For a more details on how it works, continue reading, or jump right to a specific topic of interest.

[Routed Access Datacenter](#routed-access-datacenter)   
[IP Address Management](#ip-address-management)  
[IP Address Assignment](#ip-address-assignment)  
[Tenant Isolation](#tenant-isolation)  

[Route Manager](#route-manager)  
[Service Insertion](#service-insertion)  
[Policy Based Control](#policy-based-control)  

## How it Works

### Routed Access Datacenter

To completely understand the operation of Romana, it is important to first understand the basics of a layer 3 routed access datacenter design. 

Today's modern datacenters are frequently built using a Spine/Leaf network design. The details of this approach are beyond the scope of this document, but several good resources are available [here]( http://www.cisco.com/c/en/us/products/collateral/switches/nexus-5000-series-switches/guide_c07-673997.html), [here]( https://www.arista.com/assets/data/pdf/DesignGuides/Arista-Universal-Cloud-Network-Design.pdf), and [here](http://searchdatacenter.techtarget.com/feature/Data-center-network-design-moves-from-tree-to-leaf).

These kind of layer 3 designs are popular because of their simplicity and performance and are often used as the underlay network for cloud deployments. The important point to remember about these designs is that they can take advantage of [route aggregation]( http://www.linktionary.com/r/route_aggregation.html) to further simplify their design.

Briefly stated, route aggregation is simply organizing IP addresses and configuring routers in a way that is 'topologically significant'. That is, each part of the network can only contain addresses from a specific CIDR block. By organizing the addresses hierarchically, the number of routes needed to reach every endpoint can collapse. 

The diagram below shows how route aggregation makes all endpoints in the network reachable with just a few routes configured on the leaf and spine devices. It illustrates how a 10/8 network could be partitioned across 2 Spines and 4 Leaves so that each Spine port forwards packets to one of four /10 networks (one on each Leaf) and each Leaf forward packets to one of 64 ports configured with a /16 network.

![Route Aggregation](/images/Aggregation.png)

With this configuration, each Spine device is configured with the same 4 static routes, one to each Leaf device. Each Leaf device is configured with one route to each port. The routes on the Leafs are sequential and differ across Leaf devices only by an offset in the second octet of the CIDR.

The simplicity of this design is undeniable. It is compact, intuitive and predictable and delivers high performance as well. Which is why it is so popular among large data center operators and the basis for most cloud network underlay designs. 

One constraint imposed by a routed access design is that it *requires endpoint IP addresses to conform to the route aggregation hierarchy*. 

The obvious way to relax this constraint is to update the network devices with individual host routes to the specific endpoints that do not conform to the hierarchy. 

However, updating the devices with host routes introduces the added complexity of a route distribution protocol like BGP, which requires additional skills and training. Additionally, at the limit, injecting host routes, by whatever means, will eventually overflow the capacity of the devices.

### Virtualization Hosts

Allocating a /16 CIDR and 64K IP addresses to a single port does not make a lot of sense until you realize that the host attached to this port would actually be a virtualization host that could use these available IP addresses for VMs or container running on the host.

On the [Google Compute Engine]( https://cloud.google.com/compute/) IaaS platform, when you launch a Kubernetes VM, by default it is allocated a [complete /24 network]( http://kubernetes.io/v1.0/docs/admin/networking.html#how-to-achieve-this ) so that each pod running would have an IP address within the local range. 

Using this as a simple baseline example, if you allocate a /24 to each VM, having a /16 for the entire virtualization host would accommodate up to 255 VMs.

The diagram below extends the [Routed Access Datacenter](#routed-access-datacenter) design to the virtualization hosts where the host acts as a router forwarding traffic to VMs that have been allocated /24 networks for their own use.

![Virtualization Hosts](/images/vHosts.png)

Configuring the host to route traffic is a standard feature built in to the Linux kernel. It simply has to be configured with the routes, just like a network device.

If an actual cloud datacenter were build using the assumptions from the previous example, the 16M IP addresses available in the complete 10/8 network would be allocated across 255 virtualization hosts, where each could support up to 255 VMs, with each VM supporting up to 255 IP addresses for local endpoints.

Alternative configurations are easily designed simply by changing the CIDR that is assigned to each host. A /18 to each host, would allow for four times as many hosts (up to 1K, 2^8 vs 2^10), but each host could accommodate one fourth the number of endpoints (2^16 v. 2^18).  Other variations include using only a portion of the complete 10/8 for smaller configurations, such as a 10/10 for 4M endpoints, a 10/11 for 2M endpoints, etc.

### Tenant Isolation

---
This section should probably be on a different page, but for now just including it in line.

The traditional method to maintain tenant isolation is by creating a layer 2 overlay network. These overlays use VXLAN (or similar encapsulation protocol) which supports up to 16M isolated network segments. And just like any other layer 2 segment, they are configured with a CIDR address range and gateways to routers that forward traffic to other VLANs and other networks, as necessary. 

Assigning a VXLANs to individual tenant network segments provide VLAN style isolation for every tenant segment. 

While VXLAN technology is reasonably mature and stable, the operational challenges that come with deploying them include:

1. Performance: VXLAN encapsulation requires every packet to be processed to add a header and to insert the correct field values. While line rate throughput is easily achieved, a real performance impact is seen because of the added latency introduced by multiple encapsulation/decapsulation cycles (e/d cycles). All traffic spanning hosts, even when the endpoint lie on the same segment requires an e/d cycle. For routed traffic between VLANs, packets need to be dcap'ed first, then forwarded to the router where they are decap'ed, then re-encap'ed and forwarded to the destination segment, where they again must be decap'ed, for a second complete e/d cycles.

	The diagram below shows the path packets take when OpenStack VMs on different tenant segment communicate. 

1. Visibility: VXLAN Encapsulation hides traffic inside of IP packets that can not be seen with standard network monitoring tools. Actual paths through the physical network are difficult to because endpoints inside VXLANs could be located anywhere. 

2. Managebility: 


3. Interoperability: 


4. Cost: 

![OpenStack VXLAN Routing](/images/vxlan.png)

With multiple tenant endpoints on each hypervisor there needs to be a way to ensure that only endpoints that should communicate, are able to communicate. This is achieved by configuring IP tables rules on the hypervisor that maintains a whitelist of the IP addresses that each endpoint can communicate with. [how is this done? By which component?]

Maintaining a list of all endpoints can communicate with all other endpoints is large state management problem. The problem becomes even more difficult when more advanced policies for traffic management need to be implemented.

To further simplify the design, reduce the amount of data to be maintained and to facilitate policy based traffic control, Romana encodes a tenant identifier into IP addresses. Futhermore, the tenant ID is located in the high order bits of the address so that all traffic associated with a tenant can be controlled by establishing L3 traffic management policies identified by the tenant CIDR.

End pop out

---

### IP Address Assignment and Management

To preserve the simplicity of the layer 3 network design and avoid the complexity of running a route distribution protocol for route updates, Romana provides an IP Address Management system that coordinates with cloud orchestration systems such as OpenStack and Kubernetes to provide IP addresses to new endpoints that conform to the data center's address hierarchy. 


### Route Manager

The Route Manager

### Service Insertion

The Service Insertion

### Policy Based Control
The policy controller


